---
layout: post
title: 海量信息(180T/天)top 100W提取
category: 项目
tags: 
  - rocketmq
  - hive
  - spark
imagefeature: null
mathjax: false
chart: false
comments: true
featured: false
published: false
---


研究僧：Project 3
=================

## 基本信息
*	start: 2016.01.15
*	end: 2016.03.02
*	coding: 10-15 days
*	tunning: 10 days
*	明确需求: about 2 days
*	吐槽：都不能回家过个好年……放假的时候集群坏了，非得拖到新年来

## 需求
__描述:__

数据在`rocket mq`中，一条一条的记录，每天若干条记录，每条记录包含若干字段。其中包含`ip`, `port`等字段

* 每天提取`ip`出现的`top 100W`,分别输出ip和count，从高到低排序
* 每天提起`ip`，并且包括特定端口(80,443等)的`top 100W`，分别输出ip和count，从高到低排序


__数据大小：__

* 每秒`3000万`条，即每天`180 T` 数据，每秒`2.123 GB`数据
* `76 Byte / Record`
* 如果单独提取ip，IP每天有`38T`左右的数据

哈哈，处理个一周，哥也是玩过`PB`级别数据的人了

__可用集群:__

* `59台`机器的CDH集群，每台配置如下：
	* 128G内存
	* 32 Cores CPU
	* 3块外接硬盘，每块600G (可坑爹的是，集群搭好了，盘一块没挂)
	
## 简单分析
* 首先，rocketmq的数据，应该会选择用流的形式接入，当然也可以先接入`kafka`，但最终还是流处理
* 因为要保存结果，出`top 100W`, 所以第一想法是在内存中维护一个巨大无比的`hashmap`, 每隔一段时间写一次文件
	* 128 * 60 = 7680G = 7.5 T
	* 为此，还专门看了July大神的各种大数据处理，但是他的数据确实赶不上这些数据的皮毛
	* 后来想了想，我们学大数据的时候，第一个写的代码不就是`word count`吗……
* 然后发现，做wordcount好像天然的合适。用spark streaming做窗口，设置合适的窗口和滑动窗口，每一段时间统计计数一次，而且spark streaming的wordcount那叫一个快
	* 然后做了一下资源的计算
	* 如果单独提取ip，IP每天有38T左右的数据，做`wordcount`，然后进行窗口设置
	* 假设如果10分钟统计一次结果(假设设置这么长的窗口木有问题)，则数据为：`64.85 G` 每天，算上key, value两个字段，合计100 G每天的「计算结果」
	* 滑动窗口和窗口均设置为10min，这点数据都不算什么了……
* 说干就干，接下来就是rocketmq和spark streaming的接口的事情了，将结果写入hive，到了hive里面，每天凌晨进行一个离线计算，快快滴

## 编码测试
* 开发在实验室，3台破机器组成的集群，花了几天熟悉rocketmq的接口和配置(3天，还去rocketmq的github上提了个issue，说实在的，rocketmq的文档确实不怎么全，而且是我不喜欢的java……)
* Github上学习了rocket mq的consumer如何写，如何消费，多线程
* 最后将结果写入hive
* 调通之后，根据需求写了2个hive语句，数据量大的时候，需要注意好些问题，比如`数据倾斜`，`数据清理`等，数据越多，hive的执行时间越长

## 部署
* 坑爹的事情来了，给我个集群，装好了`CDH5.4.2`, 59台机器，每台机器有3-4块盘，一块都没挂啊……光是挂盘就挂得我手抽筋了……然后写了一个脚本，利用clush一步搞定, 贴一下：


```
	#!/bin/bash
	set -ex
	
	for disk in $(ls /dev/sd* | grep -v sda | grep -v sd[a-z]1)
	do
		mountdir=$(echo $disk | cut -d "/" -f 3)
		mkdir -p /mnt/$mountdir
		mkfs.ext4 -F "$disk" # 这里直接用-F，强制格式化整个盘了
		echo "UUID=$(blkid "$disk"| cut -d "\"" -f 2)    /mnt/$mountdir    ext4    defaults   1   2" >> /etc/fstab
		mount -a
	done
```

* 然后集群各种问题来了，然后又花了好些时间修集群
	* 配置不全，hive不能用
	* spark streaming不能用
	* namenode 连接不稳定
	* 机器直接物理`宕机`，坑爹啊，居然有一台用着用着就`ping`不通
* 然后__「最最蛋疼」__的事情……能连这个集群的机器只有一台，7、8个人等着用，我又不能占着，环境配置好后，只能去别的机器，通过几次ssh连过去，然后还不能看web界面，不能看`8088`, 不能看`7180`，看yarn的日志还只能重定向打出来……这应该是最坑爹的了……而且我又不能保证一次配置好就一直能用……
* 部署是各种心酸泪

## 执行
* 写几个crontab
	* 每天凌晨0点定点执行流处理程序
	* 每天凌晨3天定时将hive里面的数据导出，然后删掉多余数据
	* 每次执行新任务前，把旧任务kill掉
	
* 流处理执行脚本，各种参数需要配置
* hive导出脚本，表按日期建立，还要清理碎片文件
* 后台执行脚本，监控执行状况，如果不小心挂了，要记得中期

## 调优
* 充分利用集群的资源，保留一小部分资源执行hive语句，以及用于系统自用
* rocketmq 的receiver 接收速度，启动若干个receiver
	* 由于每个streaming程序设定一个receiver client，所以需要启动若干个streaming 程序
	* 这里我启动了30个程序，用clush管理
* 每一个streaming程序的最佳参数，executor数量，内存，cores等。最好让每个cpu核心能同时处理不同的task，不要有cpu空闲，这样能将计算做到最大。
* hive 导出的时候选在第二天的3点，这样前一天的数据已经稳定，并且当天的streaming程序已经稳定执行，不影响hive的导出操作
* 虽然整个过程全部脚本化，并且每天定时输出结果。但是集群会不会挂，还是需要每天看一下的
	* 比如，今天又遇到了namenode 不稳定的情况，datanode试炼的情况，情况远比简单的几行代码复杂啊……
	
## 总结
* 项目已经接近尾声，每天能正常的输出2个top 100W
* 小数据的时候怎么来都行，上课的时候觉得`1G`已经挺大的了，当数据打起来，很多算法，很多优化就显得尤为必要了。
* 提前对数据量的判断，提前预估项目是否可行相当重要
* 需求下来后，如果不熟悉，真的需要先调研。带着目的调研完毕，可以选型了。选型一般定1-2个方案，好的话就可以开始coding了
* 调优很重要，各种参数让人头疼
* 还有很多值得学习的地方

> 最后嘚瑟一下，哥也是玩过「PB」级别数据的人，哈哈哈


	









